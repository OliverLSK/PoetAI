{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595ea27e-f76c-42da-9867-ff3cf8ce2684",
   "metadata": {},
   "source": [
    "## Overiew\n",
    "This project is to train and to finetune pretrained model to become PoetBot.  The goal of this project is to train chatbot that can construct short quote using themes listed in keywords with correct logic and grammar.  For example, given keywords 'friendship, love', model could generate sentences with theme of friendship and love but not necessary contain words of 'friendship' or 'love', like:\n",
    "   1. There is nothing I would not do for those who are really firends. I have no notion of loving people by halves, it is not my native.\n",
    "   2. If I had a flower for every time I thought of you... I could walk through my garden forever.\n",
    "\n",
    "This model is different with general keyword-generation model on:\n",
    "   1. Number of keywords is not fixed.\n",
    "   2. Sentence generated is random.\n",
    "\n",
    "## Related Works\n",
    "Conditional text/story generation: For task of conditional text or story generation, model usually use all keywords to compromise new sentence or story using seq2seq model with attention mechanism other pretrained model as GPT-3 and BART.\n",
    "Hierarchial story generation: This task requires to draft story line and then new story is generated based on input story line. Meaningful and detailed sentence would be required as input to generate accurate stories.\n",
    "\n",
    "## Dataset\n",
    "Quotes-500k in Kaggle, which stores qoutes with various category tags ranging from love, life to philisophy, motivation to describes quote which categories belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b53a8c-6179-4247-9069-087a00accf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-15 02:12:36,699] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0915 02:12:39.046000 11924 torch\\distributed\\elastic\\multiprocessing\\redirects.py:28] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "from transformers import TrainingArguments, pipeline\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import trl\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "import deepspeed\n",
    "from collections import Counter\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4384c73-a14b-488d-a1af-28c4744c56c9",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d658e4-a04c-40fb-baa8-09fef580a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_args = {\n",
    "    'data_path':r'data/quotes.csv',\n",
    "    'n_top': 50, # Number of Categories included\n",
    "    'token_length':64, # Maximum context length\n",
    "    'train_size':20000,\n",
    "    'valid_size':2000,\n",
    "    'test_size':10\n",
    "}\n",
    "model_args = {\n",
    "    'batch_size':1,\n",
    "    'epochs':16,\n",
    "    'num_workers':os.cpu_count(),\n",
    "    'bf16':True,\n",
    "    'fp16':False,\n",
    "    'learning_rate':0.0001,\n",
    "    'gradient_accumulation_steps':256,\n",
    "    'model_name_path':r'C:\\Users\\user\\.cache\\modelscope\\hub\\Qwen\\Qwen1___5-4B-Chat',\n",
    "    'out_dir':'outputs/qwen1___5-4B/best_model',\n",
    "}\n",
    "\n",
    "# Lora Config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj','k_proj','v_proj','o_proj'],\n",
    "    bias='none',\n",
    "    task_type='Causal_LM',\n",
    ")\n",
    "\n",
    "# Deepspeed Config - ZeRO-2 is used to offload GPU memory to CPU memory\n",
    "ds_config = {\n",
    "    'bfloat16':{\n",
    "        'enable':'auto'\n",
    "    },\n",
    "    'fp16':{\n",
    "        'enable':'auto'\n",
    "    },\n",
    "    'zero_optimization':{\n",
    "        'stage':2,\n",
    "        'offload_optimizer':{\n",
    "            'device':'cpu',\n",
    "            'pin_memory':True\n",
    "        },\n",
    "        'offload_parameter':{\n",
    "            'device':'cpu',\n",
    "            'pin_memory':True\n",
    "        },\n",
    "        'overlap_comm':True,\n",
    "        'reduce_scatter':True,\n",
    "        'reduce_bucket_size':1e8,\n",
    "        'allgather_partitions':True,\n",
    "        'allgather_bucket_size':1e8,\n",
    "        'contiguous_gradients':True\n",
    "    },\n",
    "    'gradient_accumulation_steps':1,\n",
    "    'gradient_clipping':'auto',\n",
    "    'train_batch_size':'auto',\n",
    "    'train_micro_batch_size_per_gpu':'auto',\n",
    "    'steps_per_print':1e5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08354759-e8d4-4e5c-8a0f-ca7f87868cc9",
   "metadata": {},
   "source": [
    "### Load pretrained model - Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d302b55-2881-4bfb-adbb-d74118002090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34be0c4c65d640d3aeb0c1ff96d95a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "Total parameters: 3956922880\n",
      "Total parameters: 6553600\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_args['model_name_path'])\n",
    "if model_args['bf16'] == True:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_args['model_name_path'], trust_remote_code=True).to(dtype=torch.bfloat16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args['model_name_path'], trust_remote_code=True)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_args['model_name_path'], trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args['model_name_path'], trust_remote_code=True)\n",
    "\n",
    "# Lora\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print('#'*12)\n",
    "print('Total parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "print('Total parameters: {}'.format(sum([p.numel() for p in model.parameters() if p.requires_grad])))\n",
    "print('#'*12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a9368-9f4d-428e-87a3-d92c315b4c93",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112aba83-2421-4b47-8386-6d5a887f2a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Categories:  [('love', 38805), ('life', 35074), ('inspirational', 29080), ('philosophy', 14939), ('humor', 14081), ('god', 12559), ('truth', 11827), ('wisdom', 10820), ('happiness', 10424), ('hope', 9623), ('inspirational-quotes', 9279), ('quotes', 9191), ('romance', 9121), ('faith', 8933), ('death', 8292), ('inspiration', 8163), ('success', 8127), ('writing', 7962), ('poetry', 7180), ('religion', 7155), ('knowledge', 6430), ('education', 6306), ('motivational', 6182), ('time', 6029), ('relationships', 5711), ('spirituality', 5679), ('Life', 5433), ('life-lessons', 5408), ('fear', 5338), ('motivation', 5336), ('books', 5211), ('people', 5166), ('science', 5109), ('funny', 5061), ('friendship', 5000), ('purpose', 4918), ('change', 4779), ('dreams', 4718), ('freedom', 4617), ('life-quotes', 4585), ('work', 4576), ('leadership', 4488), ('spiritual', 4434), ('women', 4400), ('christianity', 4355), ('debasish-mridha', 4343), ('peace', 4338), ('love-quotes', 4316), ('war', 4284), ('beauty', 4282)]\n",
      "Data total size:  222118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b47f2db27174153ac8b152c8824a2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08291623a0144645846e124318a50bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48d623492464ac8a3a492dd86e5df3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Intstruction:\n",
      "You are AI peom generator that turns \"Input\" categories below to poetic masterpieces.\n",
      "\n",
      "### Input:\n",
      "inspirational, inspirational-quotes\n",
      "\n",
      "###Response:\n",
      "In markets, stupid action does not have equal but severe opposite reaction.\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader(tokenizer, batch_size:int, split:str) -> DataLoader:\n",
    "    n_top=dataset_args['n_top']\n",
    "    token_len=dataset_args['token_length']\n",
    "    train_size=dataset_args['train_size']\n",
    "    valid_size=dataset_args['valid_size']\n",
    "    test_size=dataset_args['test_size']\n",
    "    file_path=dataset_args['data_path']\n",
    "\n",
    "    data_df = pd.read_csv(dataset_args['data_path']).drop('author',axis=1)\n",
    "    data_df['category'] = data_df['category'].apply(lambda x: x.split(', ') if isinstance(x, str) else x)\n",
    "    data_df = data_df[data_df['category'].notnull()]\n",
    "\n",
    "    categories = []\n",
    "    for sublist in data_df['category']:\n",
    "        if isinstance(sublist, list):\n",
    "            for item in sublist:\n",
    "                categories.append(item)\n",
    "        else:\n",
    "            categories.append(sublist)\n",
    "\n",
    "    if split == 'train':\n",
    "        print('Top Categories: ', Counter(categories).most_common(n_top))\n",
    "    top_cat =[item[0] for i, item in enumerate(Counter(categories).most_common(n_top))]\n",
    "    top_cat.sort()\n",
    "\n",
    "    ## Remove records not having any top categories tagging\n",
    "    data_df['category'] = data_df['category'].apply(lambda x: [cat if cat in x else '' for cat in top_cat])\n",
    "    data_df = data_df[data_df['category'].apply(lambda x: sum([1 for cat in x if cat!='']))>0]\n",
    "    data_df['category'] = data_df['category'].apply(lambda x: [cat for cat in x if cat!=''])\n",
    "\n",
    "    dataset = datasets.Dataset.from_pandas(data_df)\n",
    "    dataset.shuffle(8021)\n",
    "    if split == 'train':\n",
    "        print('Data total size: ', len(dataset))\n",
    "\n",
    "    def preprocess_truncate(_dataset):\n",
    "        categories = ', '.join(_dataset['category'])\n",
    "        return {\n",
    "            'input': f'### Intstruction:\\nYou are AI peom generator that turns \"Input\" categories below to poetic masterpieces.\\n\\n### Input:\\n{categories}',\n",
    "            'output': ' '.join(_dataset['quote'].split()[:token_len])\n",
    "        }\n",
    "\n",
    "    if split == 'train':\n",
    "        dataset = dataset.select(range(train_size)).map(preprocess_truncate)\n",
    "    elif split == 'valid':\n",
    "        dataset = dataset.select(range(train_size,train_size+valid_size)).map(preprocess_truncate)\n",
    "    elif split == 'test':\n",
    "        dataset = dataset.select(range(train_size+valid_size,train_size+valid_size+test_size)).map(preprocess_truncate)\n",
    "\n",
    "    ds_out = dataset\n",
    "    return ds_out\n",
    "\n",
    "ds_train = create_dataloader(tokenizer, model_args['batch_size'], 'train')\n",
    "ds_valid = create_dataloader(tokenizer, model_args['batch_size'], 'valid')\n",
    "ds_test = create_dataloader(tokenizer, model_args['batch_size'], 'test')\n",
    "\n",
    "print(ds_test[0]['input'])\n",
    "print('\\n###Response:\\n'+ds_test[0]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef080e1-4376-480e-9a64-47514f84f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    \"\"\"\n",
    "    Formatting function returning a list of samples (kind of necessary for SFT API).\n",
    "    \"\"\"\n",
    "    text = f\"{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91fd2f0d-0c7a-4dd6-aaaa-40366c4f65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(trl.SFTConfig) - Args can be set # https://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/huggingface_transformer/chapters/4_trainer.html\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{model_args['out_dir']}/logs',\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy='epoch',\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=model_args['batch_size'],\n",
    "    per_device_eval_batch_size=model_args['batch_size'],\n",
    "    num_train_epochs=model_args['epochs'],\n",
    "    bf16=model_args['bf16'],\n",
    "    fp16=model_args['fp16'],\n",
    "    dataloader_num_workers=model_args['num_workers'],\n",
    "    # gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    # gradient_checkpointing=True,\n",
    "    learning_rate=model_args['learning_rate'],\n",
    "    lr_scheduler_type='constant',\n",
    "    optim='adamw_hf',\n",
    "    deepspeed=ds_config,\n",
    "    # ddp_backend='nccl',\n",
    ")\n",
    "\n",
    "# Gradient Enable: To prevent '''RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn''' for gradient checkpoints\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# To prevent \"\"\"AttributeError: 'DummyOptim' object has no attribute 'step'\"\"\" for DeepSpeed\n",
    "from accelerate.utils import DistributedType, DeepSpeedPlugin\n",
    "training_args.distributed_state.distributed_type = DistributedType.DEEPSPEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27bd528c-0842-4613-978d-3ba7049528f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.save_checkpoint(self, out_dir='outputs/qwen1___5-4B/best_model/chkpt')>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unable to use deepspeed.initialize due to CommandError. As alternative, self-define save_checkpoint func as null\n",
    "def save_checkpoint(self, out_dir=f'{model_args['out_dir']}/chkpt'):\n",
    "    # model.save_pretrained(out_dir)\n",
    "    return\n",
    "model.save_checkpoint = save_checkpoint\n",
    "model.save_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a471bf-5602-4909-9717-3c1c36c3644b",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81abbbf9-bf23-4011-bbc4-ff7f275fbfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a818446e32b4d59b2739325ee9a5ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918976a90aa54839b12a525443c8a4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = trl.SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    formatting_func=preprocess_function,\n",
    "    packing=True,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed692318-f04b-47ee-9316-bc5d43b66650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:580: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44288' max='44288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44288/44288 6:32:41, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.655200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.602200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.544200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.495300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.438700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.296700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.237100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.184800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.126100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.071600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.047700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.982600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.961300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.921900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.879600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train time: 6:33:32.507191\n"
     ]
    }
   ],
   "source": [
    "train_start = datetime.datetime.now()\n",
    "history = trainer.train()\n",
    "train_end = datetime.datetime.now()\n",
    "print('Total train time: {}'.format(train_end-train_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eccfa39-2eb3-4a7c-af36-a5300987b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f'{model_args['out_dir']}/best_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0573f-8a02-4659-9e41-bfa062a5c612",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1469fced-d9a3-4f1e-908d-a2dc743b15f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The model 'PeftModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task='text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=dataset_args['token_length'],\n",
    "    # device_='cuda',\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "def poet_gen(data):\n",
    "    response = pipe(f\"{data['input']}\\n\\n### Response:\\n\")[0]['generated_text'][len(data['input'])+16:]\n",
    "    return {'generated_text':response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e55c3-afe5-4263-8d55-83deddb5f43b",
   "metadata": {},
   "source": [
    "Use Rouge (Recall-Oriented Understudy for Gisting Evaluation) score and BERT score to evaluate similarity of reference sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a569f68-c230-4bd1-bf58-19e265743e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function poet_gen at 0x00000272202ECAE0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ece69a902b46d99bba38c2657a7f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\user\\miniforge3\\envs\\python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f8f25f39ae4a50934438781c522f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  14%|#4        | 199M/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: rouge-1:0.0956779751419744; rouge-2:0.006666666168888927; rouge-l:0.08446445747684383; bert:0.8558026432991028\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "def model_eval(model, data):\n",
    "    rouge = Rouge()\n",
    "    # Calculate Bert score and Rouge score\n",
    "    rouge_scores = []; bert_scores = []\n",
    "    out_text = ds_test.map(poet_gen)['generated_text']\n",
    "    label_text = ds_test['output']\n",
    "    \n",
    "    r_scores = rouge.get_scores(out_text, label_text, avg=True)\n",
    "    rouge_scores.append(r_scores)\n",
    "\n",
    "    P, R, b_scores = score(out_text, label_text, lang='en')\n",
    "    bert_scores.append(np.mean(b_scores.tolist()))\n",
    "    \n",
    "    rouge_1 = np.mean([r['rouge-1']['f'] for r in rouge_scores])\n",
    "    rouge_2 = np.mean([r['rouge-2']['f'] for r in rouge_scores])\n",
    "    rouge_l = np.mean([r['rouge-l']['f'] for r in rouge_scores])\n",
    "    bert = np.mean(bert_scores)\n",
    "    print(f'Score: rouge-1:{rouge_1}; rouge-2:{rouge_2}; rouge-l:{rouge_l}; bert:{bert}')\n",
    "\n",
    "    return out_text\n",
    "\n",
    "chk = model_eval(model, ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984108d-a96c-493c-9b74-b7dbd554d5d5",
   "metadata": {},
   "source": [
    "Low in Rouge scores mean low similarity in exact words matching but high Bert scores for high contextual similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee052c-d69c-4dec-b401-42878dc9c06a",
   "metadata": {},
   "source": [
    "### Sample demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99eefa86-3cb3-4659-9f94-a606395906ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Intstruction:\n",
      "You are AI peom generator that turns \"Input\" categories below to poetic masterpieces.\n",
      "\n",
      "### Input:\n",
      "love, friendship\n",
      "\n",
      "### Response:\n",
      "I’m not sure I ever really got you. Not like you got me. But the more I try to figure you out, the clearer it gets. Like\n"
     ]
    }
   ],
   "source": [
    "def poet_gen(cats):\n",
    "    categories = ', '.join(cats)\n",
    "    return pipe(f'### Intstruction:\\nYou are AI peom generator that turns \"Input\" categories below to poetic masterpieces.\\n\\n### Input:\\n{categories}\\n\\n### Response')[0]['generated_text']\n",
    "print(poet_gen(['love','friendship']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c42c905-061d-478b-9bbb-38fe2e9d9032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Intstruction:\n",
      "You are AI peom generator that turns \"Input\" categories below to poetic masterpieces.\n",
      "\n",
      "### Input:\n",
      "love, romance\n",
      "\n",
      "### Response:\n",
      "I don't want to be the girl who never gets happy, the girl who's always pining for what could have been.\n"
     ]
    }
   ],
   "source": [
    "print(poet_gen(['love','romance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1c002c2-8080-48f5-a438-71e8b6c8ee87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Intstruction:\n",
      "You are AI peom generator that turns \"Input\" categories below to poetic masterpieces.\n",
      "\n",
      "### Input:\n",
      "god, religiontruth, wisdom\n",
      "\n",
      "### Response:\n",
      "And we say there's so much nonsense out there. But when God tells you to do something, it is a request, and if you refuse\n"
     ]
    }
   ],
   "source": [
    "print(poet_gen(['god','religion''truth','wisdom']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74635fff-6d2b-4a45-ada6-af7a92edbf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Intstruction:\n",
      "You are AI peom generator that turns \"Input\" categories below to poetic masterpieces.\n",
      "\n",
      "### Input:\n",
      "life, happiness, hope\n",
      "\n",
      "### Response:\n",
      "So long as there is this hope, joy cannot be lost.\n"
     ]
    }
   ],
   "source": [
    "print(poet_gen(['life','happiness','hope']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fbcae9f-e0cd-47a9-822c-729520a55696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release GPU memory\n",
    "del tokenizer, pipe, model, trainer\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
